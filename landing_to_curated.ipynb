{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"35f54c05-2e2c-4dca-a2e7-27ba87899407","showTitle":false,"title":""}},"outputs":[],"source":["from delta import tables\n","import pyspark.sql.functions as F"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"dd014ec5-808d-4657-900e-dda52622dcc4","showTitle":false,"title":""}},"outputs":[],"source":["# Create widgets for base parameters from ADF\n","dbutils.widgets.text(\"type\", \"\")\n","dbutils.widgets.text(\"factory\", \"\")\n","dbutils.widgets.text(\"folder\", \"\")\n","\n","DATA_TYPE = dbutils.widgets.get(\"type\")\n","FACTORY = dbutils.widgets.get(\"factory\")\n","FOLDER = dbutils.widgets.get(\"folder\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"dc0bb925-d21a-4e29-a921-bb4a7f2a5092","showTitle":false,"title":""}},"outputs":[],"source":["delta_table = \"Path_to_delta_table\" #fake path\n","tags_path = \"Path_to_tags\" #fake path\n","\n","if DATA_TYPE == \"metadata\":\n","    delta_table = delta_table + DATA_TYPE\n","    tags_path = tags_path + \"metadata/\"\n","elif DATA_TYPE == \"tags\":\n","    delta_table = delta_table + FACTORY\n","    tags_path = tags_path + f\"{FACTORY}/{FOLDER}\"\n","else:\n","    print(\"This data type is not supported\")\n","\n","# Get list of filenames in the folder\n","file_list = dbutils.fs.ls(tags_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"0be69e1e-ebeb-45fd-be32-dfb2a51c6d0d","showTitle":false,"title":""}},"outputs":[],"source":["# upsert metadata into delta table\n","def upsert_metadata(tag_df, delta_table):\n","    # Check if delta table doesnt exist\n","    if not tables.DeltaTable.isDeltaTable(spark, delta_table):\n","        # Create new delta table with new data\n","        (tag_df.write.format('delta')\n","                 .mode(\"overwrite\")\n","                 .save(delta_table))\n","    else:\n","        # Open delta table and upsert\n","        # If FQN exitsts update the values, else insert the values\n","        deltaTable = tables.DeltaTable.forPath(spark, delta_table)\n","        (deltaTable.alias(\"existingData\")\n","             .merge(source=tag_df.alias(\"newData\"), condition=\"existingData.FQN = newData.FQN\")\n","             .whenMatchedUpdateAll()\n","             .whenNotMatchedInsertAll()\n","             .execute()\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b4517ad2-df69-4279-ba0c-4884fe7cded8","showTitle":false,"title":""}},"outputs":[],"source":["# upsert sensor data into delta table\n","def upsert_sensordata(tag_df, delta_table):\n","    # Check if the delta table does not exits\n","    if not tables.DeltaTable.isDeltaTable(spark, delta_table):\n","        # Create new delta table with new data, partitioned by year, month, day and FQN\n","        (tag_df.write.format('delta')\n","                 .mode(\"overwrite\")\n","                 .partitionBy(\"Year\", \"Month\", \"Day\", \"FQN\")\n","                 .save(delta_table))\n","    else:\n","        # Open delta table and upsert\n","        # If FQN and timestamp combination exists update the values, else insert the values\n","        deltaTable = tables.DeltaTable.forPath(spark, delta_table)\n","        (deltaTable.alias(\"existingData\")\n","             .merge(source=tag_df.alias(\"newData\"), condition=\"existingData.FQN = newData.FQN AND existingData.ObsTimeStamp = newData.ObsTimeStamp\")\n","             .whenMatchedUpdateAll()\n","             .whenNotMatchedInsertAll()\n","             .execute()\n","        )"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"804c6df9-61a7-4d44-8e1e-ba1d068e9444","showTitle":false,"title":""}},"source":["### Upsert landing to curated"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4188dd59-50be-418e-bab0-aa6e90bf7a9e","showTitle":false,"title":""}},"outputs":[],"source":["# Read data from first file as a spark dataframe\n","tags_df = spark.read.format(\"csv\").load(file_list[0].path, header=True)\n","\n","# if datatype is metadata, upsert metadata\n","if DATA_TYPE == \"metadata\":\n","    upsert_metadata(tags_df, delta_table)\n","else:    \n","    # loop over every file and add the data to one spark dataframe\n","    for file in file_list[1:]:\n","        if file.size > 31:\n","            df = spark.read.format(\"csv\").load(file.path, header=True)\n","            tags_df = tags_df.union(df)\n","    # Add year, month and day column for partitioning\n","    tags_df = tags_df.withColumn(\"Year\", F.year(F.col(\"ObsTimeStamp\")))\n","    tags_df = tags_df.withColumn(\"Month\", F.month(F.col(\"ObsTimeStamp\")))\n","    tags_df = tags_df.withColumn(\"Day\", F.dayofmonth(F.col(\"ObsTimeStamp\")))\n","    # upsert senor data into delta table\n","    upsert_sensordata(tags_df, delta_table)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"60269a0c-5392-461b-91bc-505307cb8b5c","showTitle":false,"title":""}},"outputs":[],"source":["# This deletes the whole folder\n","dbutils.fs.rm(tags_path, True)"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{},"notebookName":"landing_to_curated","notebookOrigID":3100226558994264,"widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
